{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce421c26",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c72f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f122a6",
   "metadata": {},
   "source": [
    "### Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55346f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Use the known path or relative path\n",
    "if os.path.exists(r\"c:\\Users\\pompk\\Desktop\\Final-Car\\Csci-Capstone\\Dataset\"):\n",
    "    data_path = Path(r\"c:\\Users\\pompk\\Desktop\\Final-Car\\Csci-Capstone\\Dataset\")\n",
    "elif os.path.exists(r\"../Dataset\"): # Try relative path if in a different env\n",
    "    data_path = Path(r\"../Dataset\")\n",
    "else:\n",
    "    # Fallback or raise error\n",
    "    data_path = Path(\"Dataset\")\n",
    "    print(f\"Warning: Defaulting to local 'Dataset' folder. Ensure it exists.\")\n",
    "\n",
    "csv_files = list(data_path.glob(\"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {data_path}\")\n",
    "\n",
    "print(\"Loading CSV files...\")\n",
    "df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "print(f\"Loaded {len(csv_files)} files with shape {df.shape}\")\n",
    "\n",
    "# CLEANING STEPS FROM IsoForest.ipynb\n",
    "# 1. Column standardization\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# 2. String stripping\n",
    "for col in df.select_dtypes(include=\"object\").columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# 3. Replace inf/nan\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 4. Drop columns with > 30% missing\n",
    "df = df.loc[:, df.isna().mean() < 0.3]\n",
    "\n",
    "# 5. Fill numeric NaN with median\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# 6. Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 7. Drop specific columns\n",
    "drop_cols = [\n",
    "    \"fwd_header_length.1\",\n",
    "    \"fwd_avg_bytes/bulk\",\"fwd_avg_packets/bulk\",\"fwd_avg_bulk_rate\",\n",
    "    \"bwd_avg_bytes/bulk\",\"bwd_avg_packets/bulk\",\"bwd_avg_bulk_rate\"\n",
    "]\n",
    "df = df.drop(columns=drop_cols, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef41118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Layered IDS: Autoencoder (Anomaly Detection)\n",
    "# -------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Feature Selection & Correlation\n",
    "# -------------------------------\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'label' in numeric_features:\n",
    "    numeric_features.remove('label')\n",
    "\n",
    "X_unsup = df[numeric_features]\n",
    "\n",
    "# Remove highly correlated features (> 0.9)\n",
    "corr_matrix = X_unsup.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "X_unsup_clean = X_unsup.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Dropped correlated features: {len(to_drop)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(X_unsup_clean)\n",
    "\n",
    "D, K = data_normalized.shape\n",
    "print(f\"Dataset shape: {D} samples, {K} features\")\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_normalized, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Model Definition\n",
    "# -------------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = K\n",
    "hidden_dim = 16\n",
    "\n",
    "# Initialize\n",
    "model = Autoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Training Loop\n",
    "# -------------------------------\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, D, batch_size):\n",
    "        batch_data = data_tensor[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_data)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Evaluation & Threshold\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_output = model.encoder(data_tensor).numpy()\n",
    "    ae_outputs = model(data_tensor)\n",
    "    reconstruction_errors = torch.mean((ae_outputs - data_tensor) ** 2, dim=1).numpy()\n",
    "\n",
    "# Calculate statistics for threshold\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "threshold_value = mean_error + (3 * std_error)\n",
    "\n",
    "print(f\"Reconstruction Error Statistics:\")\n",
    "print(f\"Mean: {mean_error:.6f}\")\n",
    "print(f\"Std Dev: {std_error:.6f}\")\n",
    "print(f\"Threshold (mean + 3*std): {threshold_value:.6f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6️⃣ Save Artifacts\n",
    "# -------------------------------\n",
    "dump_dir = 'AutoEncoderDumps'\n",
    "if not os.path.exists(dump_dir):\n",
    "    os.makedirs(dump_dir)\n",
    "\n",
    "joblib.dump(model, os.path.join(dump_dir, 'autoencoder_model.pkl'))\n",
    "joblib.dump(scaler, os.path.join(dump_dir, 'autoencoder_scaler.pkl'))\n",
    "joblib.dump({'mean': mean_error, 'std': std_error, 'threshold': threshold_value}, os.path.join(dump_dir, 'autoencoder_threshold.pkl'))\n",
    "\n",
    "print(f\"Artifacts saved to {dump_dir}: autoencoder_model.pkl, autoencoder_scaler.pkl, autoencoder_threshold.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
